{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## K-TACC을 사용하여 다수의 주체가 포함된 뉴스 데이터 증강",
   "id": "c3af487ca0bc8ae5"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from BERT_augmentation import BertAugmentation\n",
    "from adverb_augmentation import AdverbAugmentation\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2cbd98317b5fb8",
   "metadata": {},
   "source": [
    "---\n",
    "### 라이브러리 초기화 및 함수 정의\n",
    "- `BERT`, `Adverb`를 사용하여 한국어 텍스트 데이터 증강\n",
    "- 출처: 문맥을 고려한 한국어 텍스트 데이터 증강 (Korean Text Augmentation Considering Context, K-TACC) (https://github.com/kyle-bong/K-TACC)\n",
    "- 감정 레이블을 예측을 했을 때, 여러 주체가 포함된 뉴스 데이터에서는 **감정 레이블 예측이 부정확**\n",
    "- 이러한 문제를 해결하기 위해, 예측 모델의 추가적인 Fine-tuning을 위해 **직접 선별한 100여 개의 데이터 샘플을 증강**하여 새로운 데이터셋을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e24478016d4998ba",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 초기화\n",
    "bert = BertAugmentation()\n",
    "adverb = AdverbAugmentation()\n",
    "\n",
    "# 증강 함수들\n",
    "random_masking_insertion = bert.random_masking_insertion\n",
    "adverb_gloss_replacement = adverb.adverb_gloss_replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be342edccd99fbfc",
   "metadata": {},
   "source": [
    "---\n",
    "### 데이터 증강 과정\n",
    "- **랜덤 마스킹 및 삽입 (`random_masking_insertion`)**: 원본 텍스트에서 일부단어를 무작위로 마스킹하거나 삽입하여 문장 변형\n",
    "- **부사 교체 (`adverb_gloss_replacement`)**: 텍스트에서 부사를 동의어로 교체하여 문장의 의미를 유지하면서 다양한 형태로 변환\n",
    "- 텍스트 변환 과정에서 원문의 의미가 지나치게 변하지 않도록 변환 강도를 조정하여 데이터 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d226eb0c3864ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 증강 함수 (원본 텍스트, 증강 횟수, 변환 강도)\n",
    "def augment_text(text, num_augments=5, ratio=0.2):\n",
    "    augmented_texts = []\n",
    "    for _ in range(num_augments):\n",
    "        # 증강 단계 적용\n",
    "        aug_text = random_masking_insertion(sentence=text[:512], ratio=ratio)\n",
    "        aug_text = adverb_gloss_replacement(sentence=aug_text)\n",
    "        augmented_texts.append(aug_text)\n",
    "    return augmented_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d79fad77ade8342",
   "metadata": {},
   "source": [
    "---\n",
    "### 데이터 증강 적용\n",
    "\n",
    "- **CSV 파일 읽기**: 여러 개의 주체가 포함된 뉴스 데이터 파일 read\n",
    "- **데이터 증강**: 각 문장에 대해 5번의 증강을 적용하여 새로운 데이터셋을 생성\n",
    "- **중복 제거**: 증강된 데이터에서 중복된 문장을 제거하여 데이터의 다양성을 유지\n",
    "- **기업명 강조**: 모델이 학습할 때 해당 기업에 더욱 집중할 수 있도록 텍스트에서 특정 기업명인 `SK하이닉스`와 `sk하이닉스`를 `<SK하이닉스>`로 변환.\n",
    "- **결과 저장**: 증강된 데이터를 새로운 CSV 파일로 저장\n",
    "- **증강 결과**: 94 ==> 536"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": [
    "def generate_augmented_dataset(input_file, output_file, num_augments=5, ratio=0.2):\n",
    "    # 여러개의 주체가 존재하는 뉴스 데이터 CSV 파일 불러오기\n",
    "    df = pd.read_csv(input_file, header=None, encoding='cp949')\n",
    "    print(f\"원본 데이터 수: {len(df)}\")\n",
    "\n",
    "    # 컬럼명 설정 (0번 뉴스 요약, 1번 감정 레이블)\n",
    "    df.columns = [\"description\", \"label\"]\n",
    "\n",
    "    # 증강 데이터 생성\n",
    "    augmented_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        text, label = row[\"description\"], row[\"label\"]\n",
    "        augmented_texts = augment_text(text, num_augments=num_augments, ratio=ratio)\n",
    "\n",
    "        # 원본 텍스트와 증강된 텍스트들을 하나의 리스트에 추가\n",
    "        augmented_data.append([text, label])    # 원본\n",
    "        for aug_text in augmented_texts:        # 증강 된 데이터\n",
    "            augmented_data.append([aug_text, label])\n",
    "\n",
    "    # 새로운 데이터프레임 생성\n",
    "    augmented_df = pd.DataFrame(augmented_data, columns=[\"description\", \"label\"])\n",
    "\n",
    "    # 중복 제거\n",
    "    augmented_df = augmented_df.drop_duplicates(subset=[\"description\"])\n",
    "    \n",
    "    # \"SK하이닉스\", \"sk하이닉스\" 텍스트 변환\n",
    "    augmented_df = augmented_df.applymap(lambda x: x.replace(\"SK하이닉스\", \"<SK하이닉스>\") if isinstance(x, str) else x)\n",
    "    augmented_df = augmented_df.applymap(lambda x: x.replace(\"sk하이닉스\", \"<SK하이닉스>\") if isinstance(x, str) else x)\n",
    "\n",
    "    # 결과 저장\n",
    "    augmented_df.to_csv(output_file, index=False, header=False)\n",
    "    print(f\"증강 된 데이터 셋 {output_file}. 증강 된 전체 데이터 수: {len(augmented_df)}\")"
   ],
   "id": "778bcd60311789b4"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a5a6fb6c9655bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터 수: 92\n",
      "증강 된 데이터 셋 augmented_subject_focus_data.csv. 증강 된 전체 데이터 수: 536\n"
     ]
    }
   ],
   "source": [
    "# 파일 경로 설정\n",
    "input_csv = \"input.csv\"\n",
    "output_csv = \"augmented_subject_focus_data.csv\"\n",
    "\n",
    "# 증강된 데이터 생성\n",
    "generate_augmented_dataset(input_csv, output_csv, num_augments=5, ratio=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
