{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 다수의 주체가 포함된 뉴스 감정 분석을 위해 모델의 추가적인 Fine-Tuning 수행\n",
    "\n",
    "- 뉴스 감정 예측 모델의 성능을 향상시키기 위해, **다양한 주체**가 포함된 애매한 문장을 추가적으로 학습하는 Fine-tuning을 진행\n",
    "- Fine-tuning 이전에는 아래와 같은 문장에서 감정 예측 성능이 부족했으며, 이를 보완하기 위해 학습 데이터에서 **다양한 주체가 포함된 문장**들을 선별하여 Fine-tuning 데이터셋을 생성\n",
    "- 하지만 선별된 데이터의 양이 적어 학습에 적합하지 않았기 때문에, **K-TACC**를 사용하여 데이터 증강을 진행\n",
    "\n",
    "---\n",
    "\n",
    "### Fine-tuning 이전 성능\n",
    "\n",
    "| 문장                                                                         | 실제 감정 | 예측 감정 |\n",
    "|----------------------------------------------------------------------------|-------|-------|\n",
    "| 삼성전자가 실적 발표에서 긍정적인 결과를 보였으나, SK하이닉스는 부진했다...                               | 부정    | 중립    |\n",
    "| SK하이닉스는 실적 상승을 기록했지만 삼성전자는 다소 실망스러운 실적을 발표했다...                            | 긍정    | 부정    |\n",
    "| SK하이닉스의 실적 발표 발표에서 클라우드 부문의 성장 둔화가 우려를 불러일으켰다, 아마존는 반도체 부문 시장에서 강세를 보였다... | 부정    | 긍정    |\n",
    "\n",
    "---\n",
    "\n",
    "### Fine-tuning 이후 성능\n",
    "\n",
    "| 문장                                                                         | 실제 감정 | 예측 감정 |\n",
    "|----------------------------------------------------------------------------|-------|-------|\n",
    "| 삼성전자가 실적 발표에서 긍정적인 결과를 보였으나, SK하이닉스는 부진했다...                               | 부정    | 부정    |\n",
    "| SK하이닉스는 실적 상승을 기록했지만 삼성전자는 다소 실망스러운 실적을 발표했다...                            | 긍정    | 긍정    |\n",
    "| SK하이닉스의 실적 발표 발표에서 클라우드 부문의 성장 둔화가 우려를 불러일으켰다, 아마존는 반도체 부문 시장에서 강세를 보였다... | 부정    | 부정    |\n",
    "\n",
    "---\n",
    "\n",
    "* Fine-tuning을 통해 애매한 주체가 포함된 문장에서도 모델의 감정 예측 성능 개선"
   ],
   "id": "a108fdc660031ec"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-30T05:24:09.505424Z",
     "start_time": "2024-11-30T05:23:59.745447Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 모델과 토크나이저 설정\n",
    "- KLUE BERT 모델을 금융 뉴스 감정 데이터셋에 맞춰 Fine-Tuning을 수행한 모델 사용"
   ],
   "id": "585defdbc0eb7efd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T05:24:18.083283Z",
     "start_time": "2024-11-30T05:24:09.515860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 모델 경로와 토크나이저 경로 설정\n",
    "MODEL_PATH = \"../sentiment_analysis_model\"  # 모델이 저장된 경로\n",
    "MODEL_NAME = \"klue/bert-base\"  # 모델 이름\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = TFBertForSequenceClassification.from_pretrained(MODEL_PATH)"
   ],
   "id": "c40048694b20de02",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ../sentiment_analysis_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### 데이터 전처리\n",
    "- 직접 수집한 뉴스데이터에서 다수의 주체가 포함된 문장을 선별하여 생성한 데이터셋 사용\n",
    "- `description` 열의 형식을 문자열로 변환\n",
    "- 데이터 셋을 훈련 데이터(80%)와 테스트 데이터(20%) 분리\n",
    "- 토크나이저를 사용해 **문장 정수 인코딩**, **패딩 및 트리밍 적용**, **문장 벡터화**하여 BERT 모델에 적합한 형태로 변환"
   ],
   "id": "419b2035e5ee52c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T05:24:18.472477Z",
     "start_time": "2024-11-30T05:24:18.443405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 다양한 주체가 있는 데이터 불러오기\n",
    "dataset = pd.read_csv(\"../data/augmented_subject_focus_data.csv\", encoding=\"utf-8\", header=None, names=['description', 'sentiment'])"
   ],
   "id": "8c686324108b46c6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T05:25:47.641400Z",
     "start_time": "2024-11-30T05:25:47.351264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 데이터 셋 요약 확인\n",
    "print('데이터 요약')\n",
    "dataset.info()\n",
    "\n",
    "# 'description' 컬럼을 문자열 형식으로 변환\n",
    "dataset['description'] = dataset['description'].astype(str)\n",
    "\n",
    "# \"SK하이닉스\", \"sk하이닉스\" 텍스트 변환\n",
    "dataset = dataset.applymap(lambda x: x.replace(\"SK하이닉스\", \"<SK하이닉스>\") if isinstance(x, str) else x)\n",
    "dataset = dataset.applymap(lambda x: x.replace(\"sk하이닉스\", \"<SK하이닉스>\") if isinstance(x, str) else x)\n",
    "\n",
    "# 데이터 확인\n",
    "print(dataset.head())\n",
    "\n",
    "# 새로운 토큰 추가\n",
    "new_tokens = [\"<SK하이닉스>\"]  # 추가할 새로운 토큰 리스트\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# 토큰 추가에 따라 모델의 임베딩 레이어 크기 재설정\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 데이터 전처리\n",
    "def encode_data(data, output):\n",
    "    if output :\n",
    "        print(\"데이터 전처리 과정:\")\n",
    "        for i, sentence in enumerate(data['description'].head(3)):  # 상위 3개만 출력\n",
    "            tokenized = tokenizer.encode_plus(\n",
    "                sentence,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "                return_tensors=\"tf\"\n",
    "            )\n",
    "            print(f\"문장 {i+1}: {sentence}\")\n",
    "            print(f\"토큰: {tokenizer.tokenize(sentence)}\")\n",
    "            print(f\"정수 인코딩: {tokenized['input_ids'][0].numpy()}\\n\")\n",
    "        \n",
    "    # 전체 데이터 토큰화\n",
    "    return tokenizer(\n",
    "        data['description'].tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "# 학습 데이터와 테스트 데이터 분리\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터 전처리\n",
    "train_encodings = encode_data(train_data, True)\n",
    "val_encodings = encode_data(val_data, False)"
   ],
   "id": "32e42dc43de6cd09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 요약\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 566 entries, 0 to 565\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   description  566 non-null    object\n",
      " 1   sentiment    566 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 9.0+ KB\n",
      "                                         description  sentiment\n",
      "0  삼성전자와 <SK하이닉스>가 각각 152 220 하락한 것을 비롯해 국내 반도체주가...          2\n",
      "1  이날 삼성전자와 <SK하이닉스>가 저마다 다따로 151 152 220으로 하락한 등...          2\n",
      "2  삼성전자와 <SK하이닉스>가 각각  152 220 하락한 것을 비롯해 국내 반도체주...          2\n",
      "3  반면 삼성전자와 SK SK <SK하이닉스>가 이날 각각 152 220 하락한 것을 ...          2\n",
      "4  전날 삼성전자와 <SK하이닉스>가 각각 152 220으로 하락한 것을 보고 비롯해 ...          2\n",
      "데이터 전처리 과정:\n",
      "문장 1: 삼성전자는 기대 이상의 실적을 발표했지만 <SK하이닉스>는 오히려 적기는 하지만 어느 정도로 부진한 결과를 보였다\n",
      "토큰: ['삼성전자', '##는', '기대', '이상', '##의', '실적', '##을', '발표', '##했', '##지만', '<SK하이닉스>', '는', '오히려', '적기', '##는', '하지만', '어느', '정도', '##로', '부진', '##한', '결과', '##를', '보였', '##다']\n",
      "정수 인코딩: [    2  4798  2259  3869  3658  2079  4759  2069  3913  2371  3683 32000\n",
      "   793  4312 25697  2259  3696  3875  3681  2200  6043  2470  3731  2138\n",
      "  4278  2062     3]\n",
      "\n",
      "문장 2: 지난해 아마존의 3분기 실적 발표에서 클라우드 컴퓨팅 부문 성장 둔화가 우려를 불러일으켰다 <SK하이닉스>는 새로운 기술 혁신으로 주가가 소폭 반등했다\n",
      "토큰: ['지난해', '아마존', '##의', '3', '##분', '##기', '실적', '발표', '##에서', '클라우드', '컴퓨팅', '부문', '성장', '둔화', '##가', '우려', '##를', '불러일으켰', '##다', '<SK하이닉스>', '는', '새로운', '기술', '혁신', '##으로', '주가', '##가', '소폭', '반등', '##했', '##다']\n",
      "정수 인코딩: [    2  3736 12388  2079    23  2377  2015  4759  3913 27135 11079 22371\n",
      "  4457  3877 10080  2116  4301  2138 19856  2062 32000   793  3824  3726\n",
      "  4428  6233  4960  2116 11577 19558  2371  2062     3]\n",
      "\n",
      "문장 3: 삼성전자와 <SK하이닉스>가 각각 152 220 하락한 것을 비롯해 국내 반도체주가 줄줄이 내렸다 지난주 말1일 뉴욕 증시는 충격적으로 저조한 미국 10월 고용 지표에도 불구하고 아마존 실적에 주목하며 반등했다 10월\n",
      "토큰: ['삼성전자', '##와', '<SK하이닉스>', '가', '각각', '152', '220', '하락', '##한', '것', '##을', '비롯', '##해', '국내', '반도체', '##주', '##가', '줄줄이', '내렸', '##다', '지난주', '말', '##1', '##일', '뉴욕', '증시', '##는', '충격', '##적으로', '저조', '##한', '미국', '10', '##월', '고용', '지표', '##에도', '불구', '##하고', '아마존', '실적', '##에', '주목', '##하', '##며', '반등', '##했', '##다', '10', '##월']\n",
      "정수 인코딩: [    2  4798  2522 32000   543  4235 16446 13444  4720  2470   575  2069\n",
      "  4091  2097  3739  6425  2223  2116 12355  5740  2062  8794  1041  2083\n",
      "  2210  5372  6395  2259  5326 11187 12503  2470  3666  3633  2429  4571\n",
      "  6459  6509  4784 19521 12388  4759  2170  4439  2205  2307 19558  2371\n",
      "  2062  3633  2429     3]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 모델 학습\n",
    "\n",
    "- 모델 학습 중 성능 향상 및 최적화를 위한 콜백 함수 정의 \n",
    "    - `EarlyStopping`으로 검증 정확도가 개선되지 않으면 학습 중단\n",
    "- 데이터 수에 맞춰 적당한 학습률 설정\n",
    "- 마지막 Epoch 결과 `val_loss: 0.0760`, `val_accuracy: 0.9737`"
   ],
   "id": "44bc0b7b4d63da77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T05:27:55.964944Z",
     "start_time": "2024-11-30T05:26:30.800084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 감정 레이블\n",
    "train_labels = train_data['sentiment'].values\n",
    "val_labels = val_data['sentiment'].values\n",
    "\n",
    "# TensorFlow Dataset 생성\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings), train_labels\n",
    "))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings), val_labels\n",
    "))\n",
    "\n",
    "# 배치 처리 및 데이터 셔플\n",
    "train_dataset = train_dataset.shuffle(len(train_data)).batch(16)\n",
    "val_dataset = val_dataset.batch(16)\n",
    "\n",
    "# 콜백 함수 정의 (EarlyStopping)\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', min_delta=0.001, patience=2),\n",
    "]\n",
    "\n",
    "# 모델 학습 준비 (Adam 옵티마이저, 손실 함수 설정)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(train_dataset, epochs=5, validation_data=val_dataset, callbacks=callbacks)\n",
    "\n",
    "# 학습된 모델 저장\n",
    "model.save_pretrained('../subject_focus_finetuned_model')\n",
    "tokenizer.save_pretrained(\"../subject_focus_finetuned_model\")"
   ],
   "id": "e64e0402223b7aa2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "29/29 [==============================] - 36s 521ms/step - loss: 0.7491 - accuracy: 0.7212 - val_loss: 0.4043 - val_accuracy: 0.8772\n",
      "Epoch 2/5\n",
      "29/29 [==============================] - 12s 404ms/step - loss: 0.3032 - accuracy: 0.9181 - val_loss: 0.2470 - val_accuracy: 0.9298\n",
      "Epoch 3/5\n",
      "29/29 [==============================] - 12s 404ms/step - loss: 0.1921 - accuracy: 0.9580 - val_loss: 0.1506 - val_accuracy: 0.9737\n",
      "Epoch 4/5\n",
      "29/29 [==============================] - 12s 402ms/step - loss: 0.1344 - accuracy: 0.9690 - val_loss: 0.1082 - val_accuracy: 0.9737\n",
      "Epoch 5/5\n",
      "29/29 [==============================] - 12s 404ms/step - loss: 0.1114 - accuracy: 0.9735 - val_loss: 0.0760 - val_accuracy: 0.9737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../subject_focus_finetuned_model\\\\tokenizer_config.json',\n",
       " '../subject_focus_finetuned_model\\\\special_tokens_map.json',\n",
       " '../subject_focus_finetuned_model\\\\vocab.txt',\n",
       " '../subject_focus_finetuned_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
